{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating N-Gram Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language models play a crucial role in natural language processing (NLP), powering applications such as text prediction, machine translation, and sentiment analysis. In this project, we explore key concepts behind language modeling, including probabilistic models and text preprocessing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting  the Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a function `get_book(url)` that takes in a `url` of a book's \"Plain Text UTF-8\" from a  public domain book from [Project Gutenberg](https://www.gutenberg.org/), where we will use the `requests` module to download the text and perform text preprocessing to get the book's content.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book(url):\n",
    "\n",
    "    # getting text through an HTTP request.\n",
    "    response = requests.get(url) \n",
    "\n",
    "    # delaying 0.5 seconds to follow robot.txt policy.\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    # cleaning\n",
    "    text = response.text.replace('\\r\\n','\\n') \n",
    "\n",
    "    # getting start index of book\n",
    "    start_index = re.search(r\"\\*\\*\\* START OF THE PROJECT GUTENBERG EBOOK .*? \\*\\*\\*\", text).end() \n",
    "\n",
    "    # getting last index of book\n",
    "    end_index = re.search(r\"\\*\\*\\* END OF THE PROJECT GUTENBERG EBOOK .*? \\*\\*\\*\", text).start()\n",
    "\n",
    "    # returning book text\n",
    "    return text[start_index:end_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting text from *Frankenstein; Or, The Modern Prometheus by Mary Wollstonecraft Shelley* using `get_book(url)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Frankenstein;\n",
      "\n",
      "or, the Modern Prometheus\n",
      "\n",
      "by Mary Wollstonecraft (Godwin) Shelley\n",
      "\n",
      "\n",
      " CONTENTS\n",
      "\n",
      " Letter 1\n",
      " Letter 2\n",
      " Letter 3\n",
      " Letter 4\n",
      " Chapter 1\n",
      " Chapter 2\n",
      " Chapter 3\n",
      " Chapter 4\n",
      " Chapter 5\n",
      " Chapter 6\n",
      " Chapter 7\n",
      " Chapter 8\n",
      " Chapter 9\n",
      " Chapter 10\n",
      " Chapter 11\n",
      " Chapter 12\n",
      " Chapter 13\n",
      " Chapter 14\n",
      " Chapter 15\n",
      " Chapter 16\n",
      " Chapter 17\n",
      " Chapter 18\n",
      " Chapter 19\n",
      " Chapter 20\n",
      " Chapter 21\n",
      " Chapter 22\n",
      " Chapter 23\n",
      " Chapter 24\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Letter 1\n",
      "\n",
      "_To Mrs. Saville, England._\n",
      "\n",
      "\n",
      "St. Petersburgh, Dec. 11th, 17—.\n",
      "\n",
      "\n",
      "You will rejoice to hear that no disaster has accompanied the\n",
      "commencement of an enterprise which you have regarded with such evil\n",
      "forebodings. I arrived here yesterday, and my first task is to assure\n",
      "my dear sister of my welfare and increasing confidence in the success\n",
      "of my undertaking.\n",
      "\n",
      "I am already far north of London, and as I walk in the streets of\n",
      "Petersburgh, I feel a cold northern breeze play upon my cheeks, which\n",
      "braces my nerves and fills me with delight. Do you understand this\n",
      "feeling? This breeze, which has travelled from the regions towards\n",
      "which I am advancing, gives me a foretaste of those icy climes.\n",
      "Inspirited by this wind of promise, my daydreams become more fervent\n",
      "and vivid. I try in vain to be persuaded that the pole is the seat of\n",
      "frost and desolation; it ever presents itself to my imagination as the\n",
      "region of beauty and delight. There, Margaret, the sun is for ever\n",
      "visible, its broad disk just skirting the horizon and diffusing a\n",
      "perpetual splendour. There—for with your leave, my sister, I will put\n",
      "some trust in preceding navigators—there snow and frost are banished;\n",
      "and, sailing over a calm sea, we may be wafted to a land surpassing in\n",
      "wonders and in beauty every region hitherto discovered on the habitable\n",
      "globe. Its productions and features may be without example, as the\n",
      "phenomena of the heavenly bodies undoubtedly are in those undiscovered\n",
      "solitudes. What may not be expected in a country of eternal light? I\n",
      "may there discover the wondrous power which attracts the needle and may\n",
      "regulate a thousand celestial observations that require only this\n",
      "voyage to render their seeming eccentricities consistent for ever. I\n",
      "shall satiate my ardent curiosity with the sight of a part of the world\n",
      "never before visited, and may tread a land never before imprinted by\n",
      "the foot of man. These are my enticements, and they are sufficient to\n",
      "conquer all fear of danger or death and to induce me to commence this\n",
      "laborious voyage with the joy a child feels when he embarks in a little\n",
      "boat, with his holiday mates, on an expedition of discovery up his\n",
      "native river. But supposing all these conjectures to be false, you\n",
      "cannot contest the inestimable benefit which I shall confer on all\n",
      "mankind, to the last generation, by discovering a passage near the pole\n",
      "to those countries, to reach which at present so many months are\n",
      "requisite; or by ascertaining the secret of the magnet, which, if at\n",
      "all possible, can only be effected by an undertaking such as mine.\n",
      "\n",
      "These reflections have dispelled the agitation with which I began my\n",
      "letter, and I feel my heart glow with an enthusiasm which elevates me\n",
      "to heaven, for nothing contributes so much to tranquillise the mind as\n",
      "a steady purpose—a point on which the soul may fix its intellectual\n",
      "eye. This expedition has been the favourite dream of my early years. I\n",
      "have read with ardour the accounts of the various voyages which have\n",
      "been made in the prospect of arriving at the North Pacific Ocean\n",
      "through the seas which surround the pole. You may remember that a\n",
      "history of all the voyages made for purposes of discovery composed the\n",
      "whole of our good Uncle Thomas’ library. My education was neglected,\n",
      "yet I was passionately fond of reading. These volumes were my study\n",
      "day and night, and my familiarity with them increased that regret which\n",
      "I had felt, as a child, on learning that my father’s dying injunction\n",
      "had forbidden my uncle to allow me to embark in a seafaring life.\n",
      "\n",
      "These visions faded when I perused, for the first time, those poets\n",
      "whose effusions entranced my soul and lifted it to heaven. I also\n",
      "became a poet and for one year lived in a paradise of my own creation;\n",
      "I imagined that I also might obtain a niche in the temple where the\n",
      "names of Homer and Shakespeare are consecrated. You are well\n",
      "acquainted with my failure and how heavily I bore the disappointment.\n",
      "But just at that time I inherited the fortune of my cousin, and my\n",
      "thoughts were turned into the channel of their earlier bent.\n",
      "\n",
      "Six years have passed since I resolved on my present undertaking. I\n",
      "can, even now, remember the hour from which I dedicated myself to this\n",
      "great enterprise. I commenced by inuring my body to hardship. I\n",
      "accompanied the whale-fishers on several expeditions to the North Sea;\n",
      "I voluntarily endured cold, famine, thirst, and want of sleep; I often\n",
      "worked harder than the common sailors during the day and devoted my\n",
      "nights to the study of mathematics, the theory of medicine, and those\n",
      "branches of physical science f\n"
     ]
    }
   ],
   "source": [
    "Frankenstein = get_book(\"https://www.gutenberg.org/cache/epub/84/pg84.txt\")\n",
    "\n",
    "print(Frankenstein[:5000]) # printing first 5000 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the process of splitting text into smaller units, which are called tokens. These tokens can be words, subwords, or characters, depending on the level of tokenization used. This is an important step since this is what the model processes.\n",
    "\n",
    "We will create a function `tokenize` which takes in a string, `book_string`, and returns a **list of the tokens** (words, numbers, and all punctuation) in the book such that:\n",
    "* The start of every paragraph is represented in the list with the single character `'\\x02'`.\n",
    "* The end of every paragraph is represented in the list with the single character `'\\x03'`.\n",
    "* Tokens include *no* whitespace.\n",
    "* Two or more newlines count as a paragraph break, and whitespace (e.g. multiple newlines) between two paragraphs of text do not appear as tokens.\n",
    "* All punctuation marks except `_` count as tokens, even if they are uncommon (e.g. `'@'`, `'+'`, and `'%'` are all valid tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(book_string):\n",
    "    \n",
    "    # Removing any leading or trailing white spaces\n",
    "    string = book_string.strip('\\x03').strip('\\x02').strip() \n",
    "\n",
    "    # Replacing paragraph breaks (two or more newlines) with paragraph markers\n",
    "    new_string = '\\x02 ' + re.sub(r'\\n{2,}', ' \\x03 \\x02 ', string) +\" \\x03\"\n",
    "\n",
    "    # Separating punctuation from words by adding spaces around them, then splitting into list of tokens\n",
    "    return re.sub(r'[^\\w\\s]', lambda x: ' '+x.group(0)+' ', new_string).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing *Frankenstein; Or, The Modern Prometheus by Mary Wollstonecraft Shelley*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\x02',\n",
       " 'Frankenstein',\n",
       " ';',\n",
       " '\\x03',\n",
       " '\\x02',\n",
       " 'or',\n",
       " ',',\n",
       " 'the',\n",
       " 'Modern',\n",
       " 'Prometheus',\n",
       " '\\x03',\n",
       " '\\x02',\n",
       " 'by',\n",
       " 'Mary',\n",
       " 'Wollstonecraft',\n",
       " '(',\n",
       " 'Godwin',\n",
       " ')',\n",
       " 'Shelley',\n",
       " '\\x03',\n",
       " '\\x02',\n",
       " 'CONTENTS',\n",
       " '\\x03',\n",
       " '\\x02',\n",
       " 'Letter',\n",
       " '1',\n",
       " 'Letter',\n",
       " '2',\n",
       " 'Letter',\n",
       " '3',\n",
       " 'Letter',\n",
       " '4',\n",
       " 'Chapter',\n",
       " '1',\n",
       " 'Chapter',\n",
       " '2',\n",
       " 'Chapter',\n",
       " '3',\n",
       " 'Chapter',\n",
       " '4',\n",
       " 'Chapter',\n",
       " '5',\n",
       " 'Chapter',\n",
       " '6',\n",
       " 'Chapter',\n",
       " '7',\n",
       " 'Chapter',\n",
       " '8',\n",
       " 'Chapter',\n",
       " '9']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Frankenstein_tokenized = tokenize(Frankenstein)\n",
    "\n",
    "Frankenstein_tokenized[:50] # first 50 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Unigram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A unigram language model is a model in which the probability assigned to a token is equal to the proportion of tokens in the text(corpus) that are equal to the said token. \n",
    "\n",
    "Example:\n",
    "\n",
    "```py\n",
    ">>> corpus = 'hello, I am me, when I travel, I become happy, when I am at home, I am content.'\n",
    ">>> tokenize(corpus)\n",
    "['\\x02', 'hello', ',', 'I', 'am', 'me', ',', 'when', 'I', 'travel', ',', 'I', 'become', 'happy', ',', 'when', 'I', 'am', 'at', 'home', ',', 'I', 'am', 'content', '.', '\\x03']\n",
    "```\n",
    "\n",
    "Here, there are 26 total tokens. 5 of them are equal to `'I'`, so $P(\\text{I}) = \\frac{5}{26}$. Here, the Series that the `train` method in our class below should return probability distribution of tokens:\n",
    "\n",
    "| Token    | Count | Probability |\n",
    "|----------|-------|-------------|\n",
    "| `'\\x02'` | 1   | $\\frac{1}{26}$ |\n",
    "| `'hello'` | 1   | $\\frac{1}{26}$ |\n",
    "| `','` | 5   | $\\frac{5}{26}$ |\n",
    "| `'I'` | 5   | $\\frac{5}{26}$ |\n",
    "| `'am'` | 3   | $\\frac{3}{26}$ |\n",
    "| `'me'` | 1   | $\\frac{1}{26}$ |\n",
    "| `'when'` | 2   | $\\frac{2}{26}$ |\n",
    "| `'travel'` | 1   | $\\frac{1}{26}$ |\n",
    "| `'become'` | 1   | $\\frac{1}{26}$ |\n",
    "| `'happy'` | 1   | $\\frac{1}{26}$ |\n",
    "| `'at'` | 1   | $\\frac{1}{26}$ |\n",
    "| `'home'` | 1   | $\\frac{1}{26}$ |\n",
    "| `'content'` | 1   | $\\frac{1}{26}$ |\n",
    "| `'.'` | 1   | $\\frac{1}{26}$ |\n",
    "| `'\\x03'` | 1   | $\\frac{1}{26}$ |\n",
    "\n",
    "Each probability is calculated as:  \n",
    "\n",
    "$$P(\\text{token}) = \\frac{\\text{count of token}}{\\text{total tokens}}$$  \n",
    "\n",
    "As before, the `probability` method should take in a tuple and return its probability, using the probabilities stored in `mdl`. For instance, suppose the input tuple is `(',', 'I', 'am', 'content', '.')`. Then,  \n",
    "\n",
    "$$P(\\text{, I am content .}) = P(\\text{,}) \\cdot P(\\text{I}) \\cdot P(\\text{am}) \\cdot P(\\text{content}) \\cdot P(\\text{.})$$  \n",
    "\n",
    "Substituting the probabilities from our table:  \n",
    "\n",
    "$$P(\\text{, I am content.}) = \\frac{5}{26} \\cdot \\frac{5}{26} \\cdot \\frac{3}{26} \\cdot \\frac{1}{26} \\cdot \\frac{1}{26}$$  \n",
    "\n",
    "The `sample` method accounts for the fact that not all tokens are equally likely to be sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramLM(object):\n",
    "    \n",
    "    def __init__(self, tokens):\n",
    "\n",
    "        self.mdl = self.train(tokens)\n",
    "    \n",
    "    def train(self, tokens):\n",
    "        return pd.Series(tokens).value_counts() / len(tokens)\n",
    "    \n",
    "    def probability(self, words):\n",
    "        try:\n",
    "            if len(words)==1:\n",
    "                return self.mdl.loc[words]\n",
    "            else:\n",
    "                return float((self.mdl.loc[list(words)]).prod())\n",
    "        except KeyError as e:\n",
    "            return 0\n",
    "        \n",
    "    def sample(self, M):\n",
    "        return ' '.join(np.random.choice(self.mdl.index,M,p=self.mdl.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we try our unigram model using the same Frankenstein tokenized text/corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",                 0.056607\n",
      "the               0.044651\n",
      "and               0.033539\n",
      ".                 0.033345\n",
      "I                 0.032500\n",
      "                    ...   \n",
      "salvation         0.000011\n",
      "timorous          0.000011\n",
      "irreproachable    0.000011\n",
      "indecent          0.000011\n",
      "thinks            0.000011\n",
      "Name: count, Length: 7416, dtype: float64\n",
      "\n",
      "Expected Probability of \"my dear\": 1.4072164782064333e-05\n",
      "\n",
      "my , liberty as knowledge which . not accounted like , My Ingolstadt eyes Under sufficiency was , way the happen court league had myself by and ever , long of could but . between unfit tears but little to desirous you scenes your fuel arranged of that . , one the . sudden remain and that having . the duties Well in great Empires_ but the had fill “ At detestation and and I\n"
     ]
    }
   ],
   "source": [
    "# initializing unigram object with tokenized text that will be trained to probability distribution df(mdl df)\n",
    "unigram = UnigramLM(Frankenstein_tokenized) \n",
    "\n",
    "# probability distribution of frankenstein text\n",
    "print(unigram.mdl)\n",
    "\n",
    "print()\n",
    "\n",
    "# gives probability of input tokenized text\n",
    "print(f'Expected Probability of \"my dear\": {unigram.probability(('my','dear'))}') \n",
    "\n",
    "print()\n",
    "# Generates a random sequence of 75 words.\n",
    "print(unigram.sample(75) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an N-gram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the issue with the previous model is that it does not condition of previous words. It assumes independence between words in a sentence, which is wrong. Now, we will perform conditional probability to make sure sequences of words make sense.\n",
    "\n",
    "\n",
    "The model would assume that the probability that a token occurs depends only on the previous $N-1$ tokens, rather than all previous tokens. That is:\n",
    "\n",
    "$$P(w_n|w_1,\\ldots,w_{n-1}) = P(w_n|w_{n-(N-1)},\\ldots,w_{n-1})$$\n",
    "\n",
    "So, in an N-Gram language model, we get to choose, $N$. For any $N$, the resulting N-Gram model looks at the previous $N-1$ tokens when computing probabilities. So, the previous model we made was a 1-Gram model known as a unigram.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Trigram Model Example\n",
    "\n",
    "When $N=3$, we have a \"trigram\" model. This model looks at the previous $N-1 = 2$ tokens when computing probabilities if possible.\n",
    "\n",
    "Consider the tuple `('when', 'I', 'travel', 'I', 'become', 'happy')`. Under the trigram model, the probability of this sentence is computed as follows:\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "P(\\text{when I travel I become happy}) &= P(\\text{when}) \\cdot P(\\text{I | when}) \\cdot P(\\text{travel | when I}) \\cdot \\\\\n",
    "&\\quad P(\\text{I | I travel}) \\cdot P(\\text{become | travel I}) \\cdot P(\\text{happy | I become})\n",
    "\\end{aligned}\n",
    "\\]\n",
    "Note that the first few tokens may not have 2 tokens before it. We will take that into account and go to previous n-gram dataframes when computing these special cases. For example, for the first word, we will go back to the unigram model because it does not have anything before it.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "#### N-Grams\n",
    "\n",
    "When working with a training text and implementing the `probability` method to compute the probabilities of sequences of words, we work with \"chunks\" of $N$ tokens at a time.\n",
    "\n",
    "**Definition:** The **N-Grams of a text** are a list of tuples containing sliding windows of length $N$. A sliding window of size N moves over the text one token at a time, extracting continuous sequences of N words.\n",
    "\n",
    "For instance, the trigrams in the sentence `'when I travel I become happy'` are:\n",
    "\n",
    "```py\n",
    "[('when', 'I', 'travel'), ('I', 'travel', 'I'), ('travel', 'I', 'become'), ('I', 'become', 'happy')]\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Computing N-Gram Probabilities\n",
    "\n",
    "In our trigram model above, we computed $P(\\text{when I travel I am happy})$ as a product of conditional probabilities. These conditional probabilities are the result of **training** our N-Gram model on a training corpus.\n",
    "\n",
    "To train an N-Gram model, we must compute a conditional probability for every $N$-token sequence in the corpus. For instance, when we are training a trigram model, for every 3-token sequence $w_1, w_2, w_3$, we must compute $P(w_3 | w_1, w_2)$. To do so, we use the definition of conditional probability:\n",
    "\n",
    "$$P(w_3 | w_1, w_2) = \\frac{C(w_1, w_2, w_3)}{C(w_1, w_2)}$$\n",
    "\n",
    "where $C(w_1, w_2, w_3)$ is the number of occurrences of the trigram sequence $w_1, w_2, w_3$ in the training corpus and $C(w_1, w_2)$ is the number of occurrences of the bigram sequence  $w_1, w_2$ in the training corpus.\n",
    "\n",
    "In general, for any $N$, conditional probabilities are computed by dividing the counts of N-Grams by the counts of the (N-1)-Grams they follow. \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### The `NGramLM` Class \n",
    "\n",
    "1. **Creating an N-Gram Model**  \n",
    "   - `NGramLM` takes in a **list of tokens**  and a number **N**.  \n",
    "   - `N` must be **less than or equal to the total number of tokens**.  \n",
    "\n",
    "2. **Generating N-Grams**  \n",
    "   - The method `create_ngrams` extracts **overlapping N-word sequences** (tuples of length N) from the token list.  \n",
    "   - These N-grams are then **used to train the model**.  \n",
    "\n",
    "3. **Training the Model**  \n",
    "   - The `train` method stores the N-gram data in a **DataFrame** with three columns:  \n",
    "     - **`ngram`** → The actual N-gram (e.g., `(\"I\", \"become\", \"happy\")`).  \n",
    "     - **`n1gram`** → The **first (N-1) words** of the N-gram (e.g., `(\"I\", \"become\")`).  \n",
    "     - **`prob`** → The **probability** of the N-gram appearing in the text.  \n",
    "\n",
    "4. **Storing Previous Models**  \n",
    "   - The class keeps track of **(N-1)-gram models** inside `prev_mdl`.  \n",
    "   - This helps calculate **probabilities for the first word(s) of a sentence**.  \n",
    "\n",
    "The N-Gram LM consists of probabilities of the form\n",
    "\n",
    "$$P(w_n|w_{n-(N-1)},\\ldots,w_{n-1})$$\n",
    "\n",
    "which we estimate by  \n",
    "\n",
    "$$\\frac{C(w_{n-(N-1)}, w_{n-(N-2)}, \\ldots, w_{n-1}, w_n)}{C(w_{n-(N-1)}, w_{n-(N-2)}, \\ldots, w_{n-1})}$$\n",
    "\n",
    "for every N-Gram that occurs in the corpus. \n",
    "\n",
    "Example:\n",
    "\n",
    "```py\n",
    ">>> corpus = 'hello, I am me, when I travel, I become happy, when I am at home, I am content.'\n",
    ">>> tokens = tokenize(corpus)\n",
    ">>> tokens\n",
    "['\\x02', 'hello', ',', 'I', 'am', 'me', ',', 'when', 'I', 'travel', ',', 'I', 'become', 'happy', ',', 'when', 'I', 'am', 'at', 'home', ',', 'I', 'am', 'content', '.', '\\x03']\n",
    ">>> model = NGrams(3, tokens)\n",
    "```\n",
    "\n",
    "\n",
    "To compute $P(\\text{travel | when I})$, we must find the number of occurrences of `'when I travel'` in the training corpus, and divide it by the number of occurrences of `'I travel'` in the training corpus. `'when I travel'` occurred exactly once in the training corpus, while `'when I'` occurred twice, so,\n",
    "\n",
    "$$P(\\text{travel | when I}) = \\frac{C(\\text{when I travel})}{C(\\text{when I})} = \\frac{1}{2}$$\n",
    "\n",
    "To store the conditional probabilities of all N-Grams, we will use a DataFrame with three columns as shown below:\n",
    "\n",
    "|  | ngram                    | n1gram              |     prob |\n",
    "|-------|-------------------------|--------------------|---------:|\n",
    "| 0     | (',', 'I', 'become')     | (',', 'I')          | 0.333333 |\n",
    "| 1     | ('I', 'am', 'at')        | ('I', 'am')         | 0.333333 |\n",
    "| 2     | ('I', 'am', 'content')   | ('I', 'am')         | 0.333333 |\n",
    "| 3     | ('I', 'am', 'me')        | ('I', 'am')         | 0.333333 |\n",
    "| 4     | ('when', 'I', 'am')      | ('when', 'I')       | 0.5      |\n",
    "| 5     | ('when', 'I', 'travel')  | ('when', 'I')       | 0.5      |\n",
    "| 6     | (',', 'I', 'am')         | (',', 'I')          | 0.666667 |\n",
    "| 7     | ('\\x02', 'hello', ',')   | ('\\x02', 'hello')   | 1        |\n",
    "| 8     | (',', 'when', 'I')       | (',', 'when')       | 1        |\n",
    "| 9   | ('I', 'become', 'happy') | ('I', 'become')     | 1        |\n",
    "| 10    | ('I', 'travel', ',')     | ('I', 'travel')     | 1        |\n",
    "| 11    | ('am', 'at', 'home')     | ('am', 'at')        | 1        |\n",
    "| 12    | ('am', 'content', '.')   | ('am', 'content')   | 1        |\n",
    "| 13    | ('am', 'me', ',')        | ('am', 'me')        | 1        |\n",
    "| 14    | ('at', 'home', ',')      | ('at', 'home')      | 1        |\n",
    "| 15    | ('become', 'happy', ',') | ('become', 'happy') | 1        |\n",
    "| 16    | ('content', '.', '\\x03') | ('content', '.')    | 1        |\n",
    "| 17    | ('happy', ',', 'when')   | ('happy', ',')      | 1        |\n",
    "| 18    | ('hello', ',', 'I')      | ('hello', ',')      | 1        |\n",
    "| 19    | ('home', ',', 'I')       | ('home', ',')       | 1        |\n",
    "| 20   | ('me', ',', 'when')      | ('me', ',')         | 1        |\n",
    "| 21    | ('travel', ',', 'I')     | ('travel', ',')     | 1        |\n",
    "\n",
    "- Each row contains the conditional probability for its corresponding trigram. For instance the first row in the dataframe shows that the probability of the trigram `(',', 'I', 'become')` conditioned on the bigram `(',', 'I')` is 1/3, as we computed above. \n",
    "- Note that many of the above conditional probabilities are equal to 1 because many trigrams and their corresponding bigrams each appeared only once, and $\\frac{1}{1} = 1$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLM(object):\n",
    "    \n",
    "    def __init__(self, N, tokens):\n",
    "        \n",
    "        self.N = N\n",
    "\n",
    "        # Create N-grams from tokens\n",
    "        ngrams = self.create_ngrams(tokens)\n",
    "\n",
    "        self.ngrams = ngrams\n",
    "        # Train the model (calculate probabilities)\n",
    "        self.mdl = self.train(ngrams)\n",
    "        # Raise an exception if N is intially set to less than 2.\n",
    "        if N < 2:\n",
    "            raise Exception('N must be greater than 1')\n",
    "        \n",
    "        # Initialize lower-order models for first probabilities \n",
    "        elif N == 2:\n",
    "            self.prev_mdl = UnigramLM(tokens)\n",
    "        else:\n",
    "            self.prev_mdl = NGramLM(N-1, tokens)\n",
    "\n",
    "    def create_ngrams(self, tokens):\n",
    "        # creates all possible ngrams from tokens\n",
    "        lst = []\n",
    "        for i in range(len(tokens)-(self.N - 1)):\n",
    "            lst.append(tuple(tokens[i:self.N+i]))\n",
    "        return lst\n",
    "        \n",
    "    def train(self, ngrams):\n",
    "\n",
    "        # Count occurrences of N-grams and (N-1)-grams\n",
    "        helper_func = lambda x: [tuple(i[:-1]) for i in x]\n",
    "        ngram_counts = pd.Series(ngrams).value_counts()\n",
    "\n",
    "        n1gram=helper_func(ngrams)\n",
    "        n1gram_counts = pd.Series(n1gram).value_counts()\n",
    "\n",
    "        # Compute probability of each N-gram\n",
    "        probs = {}\n",
    "        for i,j in ngram_counts.items():\n",
    "            match = tuple(i[:-1]) \n",
    "            probs[i]= j / n1gram_counts[match]\n",
    "        \n",
    "        # Create a probability distribution DataFrame \n",
    "        df =pd.DataFrame({\"ngram\": probs.keys(), \"prob\": probs.values()})\n",
    "        df['n1gram'] = df['ngram'].apply(lambda i: tuple(i[:-1]) )\n",
    "        return df.iloc[:,[0,2,1]].drop_duplicates().sort_values(['prob','ngram','n1gram'])\n",
    "    \n",
    "    def probability(self, words):\n",
    "\n",
    "        # Generate possible N-grams from input words\n",
    "        my_ngrams = []\n",
    "        for i in range(len(words),0,-1):\n",
    "            if len(words[:i]) > self.N:\n",
    "                my_ngrams.append(tuple(words[:i][-self.N:]))\n",
    "            else:\n",
    "                my_ngrams.append(tuple(words[:i]))\n",
    "        final_prob= 1\n",
    "        obj = self\n",
    "        for i in my_ngrams:\n",
    "            n = len(i)\n",
    "\n",
    "            # Navigate back to the appropriate N-gram model if needed\n",
    "            while obj.N!=n:\n",
    "                obj = obj.prev_mdl\n",
    "                if isinstance(obj,UnigramLM):\n",
    "                    break\n",
    "            if n>1:\n",
    "                df = obj.mdl\n",
    "                df = df[df['ngram']==i]\n",
    "                if df.shape[0]==0:\n",
    "                    return 0 # If the N-gram is not found, return zero probability\n",
    "                final_prob *= df['prob'].iloc[0]\n",
    "                \n",
    "            else:\n",
    "                final_prob *= obj.probability(i)\n",
    "            \n",
    "        return final_prob\n",
    "       \n",
    "    def sample(self, M):\n",
    "\n",
    "        out = ['\\x02'] # Start symbol\n",
    "        for i in range(M-1):\n",
    "            beg = out[-(self.N - 1):] # Get the last (N-1) words as context\n",
    "            n_needed = len(beg)+1\n",
    "            obj = self\n",
    "            # Navigate back to the appropriate N-gram model if needed\n",
    "            while obj.N!=n_needed:\n",
    "                    obj = obj.prev_mdl\n",
    "                    if isinstance(obj,UnigramLM):\n",
    "                        break\n",
    "            df = obj.mdl\n",
    "            df = df[df['n1gram']==tuple(beg)]\n",
    "            if df.shape[0]==0:\n",
    "                out.append(\"\\x03\") # End symbol if no match found\n",
    "                continue\n",
    "             # Sample next word based on probabilities\n",
    "            props = df.set_index('ngram')[\"prob\"]\n",
    "            next_word = np.random.choice(props.index.to_numpy(),1,p=props.to_numpy())[0][-1]\n",
    "            out.append(next_word)\n",
    "        out.append(\"\\x03\") # End symbol\n",
    "        return ' '.join(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we try our n-gram model using the same Frankenstein tokenized text/corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\x02', 'Frankenstein', ';'),\n",
       " ('Frankenstein', ';', '\\x03'),\n",
       " (';', '\\x03', '\\x02'),\n",
       " ('\\x03', '\\x02', 'or'),\n",
       " ('\\x02', 'or', ','),\n",
       " ('or', ',', 'the'),\n",
       " (',', 'the', 'Modern'),\n",
       " ('the', 'Modern', 'Prometheus'),\n",
       " ('Modern', 'Prometheus', '\\x03'),\n",
       " ('Prometheus', '\\x03', '\\x02'),\n",
       " ('\\x03', '\\x02', 'by'),\n",
       " ('\\x02', 'by', 'Mary'),\n",
       " ('by', 'Mary', 'Wollstonecraft'),\n",
       " ('Mary', 'Wollstonecraft', '('),\n",
       " ('Wollstonecraft', '(', 'Godwin'),\n",
       " ('(', 'Godwin', ')'),\n",
       " ('Godwin', ')', 'Shelley'),\n",
       " (')', 'Shelley', '\\x03'),\n",
       " ('Shelley', '\\x03', '\\x02'),\n",
       " ('\\x03', '\\x02', 'CONTENTS')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ngram = NGramLM(3, Frankenstein_tokenized)\n",
    "\n",
    "# all possible 3-ngrams\n",
    "ngram.ngrams[:20] # first 20 3-ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>n1gram</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30847</th>\n",
       "      <td>(,, and, Daniel)</td>\n",
       "      <td>(,, and)</td>\n",
       "      <td>0.001058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16345</th>\n",
       "      <td>(,, and, De)</td>\n",
       "      <td>(,, and)</td>\n",
       "      <td>0.001058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24203</th>\n",
       "      <td>(,, and, Ernest)</td>\n",
       "      <td>(,, and)</td>\n",
       "      <td>0.001058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10497</th>\n",
       "      <td>(,, and, Greenwich)</td>\n",
       "      <td>(,, and)</td>\n",
       "      <td>0.001058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36911</th>\n",
       "      <td>(,, and, Henry)</td>\n",
       "      <td>(,, and)</td>\n",
       "      <td>0.001058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4771</th>\n",
       "      <td>(”, but, I)</td>\n",
       "      <td>(”, but)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27416</th>\n",
       "      <td>(”, interrupted, the)</td>\n",
       "      <td>(”, interrupted)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>(”, she, said)</td>\n",
       "      <td>(”, she)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65315</th>\n",
       "      <td>(”, were, not)</td>\n",
       "      <td>(”, were)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51053</th>\n",
       "      <td>(”, you, will)</td>\n",
       "      <td>(”, you)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73142 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ngram            n1gram      prob\n",
       "30847       (,, and, Daniel)          (,, and)  0.001058\n",
       "16345           (,, and, De)          (,, and)  0.001058\n",
       "24203       (,, and, Ernest)          (,, and)  0.001058\n",
       "10497    (,, and, Greenwich)          (,, and)  0.001058\n",
       "36911        (,, and, Henry)          (,, and)  0.001058\n",
       "...                      ...               ...       ...\n",
       "4771             (”, but, I)          (”, but)  1.000000\n",
       "27416  (”, interrupted, the)  (”, interrupted)  1.000000\n",
       "1796          (”, she, said)          (”, she)  1.000000\n",
       "65315         (”, were, not)         (”, were)  1.000000\n",
       "51053         (”, you, will)          (”, you)  1.000000\n",
       "\n",
       "[73142 rows x 3 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ngram probability distribution when n=3\n",
    "ngram.mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.14195662848725e-05"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probability of greatest practical\n",
    "ngram.probability(('greatest','practical'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0002 It was on a bed , hardly conscious of what they said , “ will you have yourself described to be informed of the fire of love to bestow . \u0003 \u0002 But I thought of the accumulated ice , among merchants and seamen . Yet at the bottom of the grave - worms crawling in the least interest me , and the spirits who assist my vengeance in his boat , with a feeble voice , “ how kind , how very ill , all capacity of bestowing animation upon lifeless matter , ” said I was dependent on none and related to none . ‘ Who are you ? I have no one was too potent , and but one vast hand was already dusk before we thought of my early years . You were thrown into prison the very stars themselves being witnesses and arranging my chemical apparatus . \u0003 \u0002 I mentioned before , which were at first ; and the rest necessary for my guest which this tale of misery , according as they roared and dashed at my bedside ; its astounding horror would be pursuit ? Who was I really as mad as the peasants ; and you will , destroy the lamb and the scene of waters , woods , high and snowy mountains were before me with strange feelings . \u0003 \u0002 No one can conjecture to what place he has already recovered his spirits , if you do not intend to do in the greatest affection . He is now much restored ; and , as a new scene to us ; and when I first saw the ocean appeared at a place of safety . Felix seemed peculiarly happy and beloved as I plunged yet deeper in the immeasurable waters that roared and buffeted around me . \u0003 \u0002 Saying this , he found on the appearance of the threat of the fingers was mentioned I remembered also the names of the customs of the weight of despair . At that age I became nervous to a hiding - places . My place of sadness in the house of mourning and to say , with a free man . \u0003 \u0002 “ The cottagers arose the next day when , by the Austrian government . A few days at Rotterdam , whence we proceeded to Paris and delivered himself up to assist him , ‘ and I hope , to call up the instrument of mischief disturbs me ; all pleasure seemed to me , and I longed to discover from his face , my heart , to dress my food , and her pale and distorted in its general relations , and he turned his thoughts towards the same nature as man . As soon as night obscured the shapes of the monster on whom the night in which she floated . Our situation was somewhat dangerous , especially the two younger cottagers exhibited towards their venerable \u0003\n"
     ]
    }
   ],
   "source": [
    "# Generates a text of 500 words through the dataframe of probability distributions. \n",
    "print(ngram.sample(500))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc80",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
